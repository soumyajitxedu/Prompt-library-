Briefing Document: Large Language Model Prompt Engineering
Date: 26th May 2024

Subject: Review of provided sources on Prompt Engineering and Large Language Models (LLMs)

Executive Summary:

This briefing document synthesises key information from the provided sources on prompt engineering and Large Language Models. It covers foundational concepts such as how LLMs function by predicting tokens, different prompting strategies (zero-shot, few-shot, system, contextual, role-based), and the crucial role of parameters like temperature, top-K, and top-P in controlling output. The documents also highlight the practical applications of prompt engineering, particularly in code generation, explanation, translation, and debugging, as well as the increasing integration of LLMs into productivity tools for various professional roles. Finally, the concept of LLM security and the use of structured reasoning protocols are introduced.

Main Themes and Key Ideas:

LLMs as Token Predictors:
The fundamental mechanism of an LLM is predicting the next "token" (group of letters) based on the preceding text. This process continues until a maximum token limit is reached or a stop token is encountered.
"That’s pretty much what an LLM is doing—but at scale. Instead of text on your phone, an LLM works to predict the next best group of letters, which are called “tokens.”" (GitHub Blog)
Prompting Strategies:
Different methods exist for structuring prompts to guide LLM output effectively.
Zero-shot prompting: Providing a prompt without any examples. The model relies on its pre-training to understand the task. (22365_3_Prompt Engineering_v7 (1).pdf, p. 13)
One-shot & Few-shot prompting: Providing one or a few examples within the prompt to demonstrate the desired input/output format or task. This helps the model learn the pattern. (22365_3_Prompt Engineering_v7 (1).pdf, p. 15)
Table 2 in "22365_3_Prompt Engineering_v7 (1).pdf" provides a clear example of few-shot prompting for parsing pizza orders to JSON.
System Prompting: Providing additional instructions or constraints on how the output should be returned, often related to format or style. (22365_3_Prompt Engineering_v7 (1).pdf, p. 19)
"System prompts can be useful for generating output that meets specific requirements. The name ‘system prompt’ actually stands for ‘providing an additional task to the system’." (22365_3_Prompt Engineering_v7 (1).pdf, p. 19)
Role Prompting: Instructing the LLM to adopt a specific persona or role to influence the tone, style, and content of its response. This is demonstrated extensively in the awesome-chatgpt-prompts source with examples like "Act as a Linux Terminal," "Act as a Travel Guide," "Act as a Dentist," etc.
Contextual Prompting: Providing relevant context or background information to the LLM to improve the accuracy and relevance of its output. This includes incorporating external data or information from other documents.
"Irrelevant information in an LLM’s context decreases its accuracy." (GitHub Blog)
The Gemini for Google Workspace guide highlights using "@file name" to include information from specific documents as context.
Controlling LLM Output Parameters:
Parameters such as Temperature, Top-K, and Top-P significantly influence the randomness and diversity of the LLM's output.
Temperature: Controls the degree of randomness. Lower values lead to more deterministic and focused output, while higher values increase creativity and unexpected results. (22365_3_Prompt Engineering_v7 (1).pdf, p. 10)
"Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results." (22365_3_Prompt Engineering_v7 (1).pdf, p. 10)
A temperature of 0 is described as "greedy decoding," always selecting the highest probability token.
Top-K: Limits the number of highest probability tokens considered at each step of generation. Setting Top-K to 1 makes Temperature and Top-P irrelevant. (22365_3_Prompt Engineering_v7 (1).pdf, p. 10)
Top-P: Filters tokens based on cumulative probability. It considers the smallest set of tokens whose cumulative probability exceeds the Top-P value. Setting Top-P to 0 or a very small value typically results in only the most probable token being considered. (22365_3_Prompt Engineering_v7 (1).pdf, p. 10)
The sources provide general starting points for these parameters depending on the desired creativity level (e.g., Temperature .2, Top-P .95, Top-K 30 for relatively coherent results; Temperature .9, Top-P .99, Top-K 40 for creative results). (22365_3_Prompt Engineering_v7 (1).pdf, p. 10)
A warning is included about the "repetition loop bug," which can be exacerbated by inappropriate parameter settings. (22365_3_Prompt Engineering_v7 (1).pdf, p. 10)
Code Prompting:
LLMs can be used effectively for various code-related tasks.
Writing Code: Generating code snippets based on natural language descriptions. (22365_3_Prompt Engineering_v7 (1).pdf, p. 42) Table 16 demonstrates generating a Bash script to rename files.
Explaining Code: Providing descriptions and breakdowns of existing code. (22365_3_Prompt Engineering_v7 (1).pdf, p. 44)
Translating Code: Converting code from one programming language to another. (22365_3_Prompt Engineering_v7 (1).pdf, p. 46) Table 18 shows translating Bash code to Python.
Debugging and Reviewing Code: Identifying errors in code and suggesting improvements. (22365_3_Prompt Engineering_v7 (1).pdf, p. 48) Table 19 demonstrates using an LLM to debug a Python script and suggest improvements.
A crucial caution is raised: "However, since LLMs can’t reason, and repeat training data, it’s essential to read and test your code first." (22365_3_Prompt Engineering_v7 (1).pdf, p. 43)
Integration into Productivity Tools and Professional Roles:
LLMs, such as Gemini for Google Workspace, are being integrated into common applications (Gmail, Docs, Sheets, etc.) to serve as AI-powered assistants. (gemini-for-google-workspace-prompting-guide-101.pdf, p. 5)
Prompting can be used to automate tasks and enhance workflows across various professional roles, including administrative support, communications, customer service, HR, marketing, project management, sales, and small business. (gemini-for-google-workspace-prompting-guide-101.pdf, Table of Contents)
Examples include drafting emails, creating presentations, analysing data in spreadsheets, generating marketing copy, and creating customer journey maps.
Advanced Prompting Concepts and Techniques:
The "22365_3_Prompt Engineering_v7 (1).pdf" mentions techniques like Step-back prompting, Chain of Thought (CoT), Self-consistency, Tree of Thoughts (ToT), ReAct (reason & act), and Automatic Prompt Engineering (pp. 25-40). These are more advanced strategies for improving the reasoning and performance of LLMs.
The "a-practical-guide-to-building-agents.pdf" introduces the concept of "Agents," which utilise LLMs in a loop with tool calls until an exit condition is met.
The "awesome-chatgpt-prompts" source showcases role-playing prompts as a way to guide the LLM's output style and content. It also includes examples of prompts for specific technical tasks like acting as a Linux terminal, SQL terminal, Regex Generator, and Python Interpreter.
The Structured Iterative Reasoning Protocol (SIRP) is mentioned as a way to break down complex problems and guide the LLM's reasoning process through iterative steps and self-reflection. (awesome-chatgpt-prompts)
LLM Security:
The "awesome-chatgpt-prompts" source includes a prompt for acting as a "Large Language Models Security Specialist," highlighting the importance of identifying vulnerabilities in LLMs and mitigating risks like unauthorized data disclosure or prompt injection attacks.
Important Facts:

LLMs work by predicting tokens, not by truly understanding or reasoning in a human-like way.
Prompting strategies range from simple zero-shot to more complex few-shot, system, contextual, and role-based methods.
Parameters like Temperature, Top-K, and Top-P are critical for controlling the variability and focus of LLM output.
Inappropriate parameter settings can lead to issues like the "repetition loop bug."
LLMs are increasingly integrated into productivity software to assist users in various professional tasks.
When using LLMs for code generation, it is essential to manually review and test the output.
Advanced prompting techniques and agent-based systems are being developed to improve LLM capabilities.
Ensuring the security and robustness of LLMs against vulnerabilities is an important area of focus.
Conclusion:

The provided sources offer a comprehensive overview of prompt engineering, from fundamental principles to practical applications and advanced techniques. Understanding how LLMs function, mastering different prompting strategies, and effectively controlling output parameters are crucial for leveraging the power of these models. The integration of LLMs into everyday tools is expanding their utility across various professional domains. While LLMs offer significant potential, it is important to be aware of their limitations and consider security implications. Continued development in prompt engineering and related concepts aims to enhance the capabilities and reliability of LLMs.

